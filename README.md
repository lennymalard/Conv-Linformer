# Conv-Linformer: Boosting Linformerâ€™s Performance with Convolution in Small-Scale Settings

This repository contains the code and experiments from my application to the **Eastern European Machine Learning (EEML) Summer School**. The project is divided into two parts:

1. A **systematic study of Linformer's behavior** in resource-constrained settings (limited data, long sequences, varying learning rates).
2. A proposed variation â€” **Conv-Linformer** â€” designed to improve stability and performance using 1D convolutions in the attention mechanism.

---

## ðŸ§  Motivation

The [Linformer](https://arxiv.org/abs/2006.04768) reduces the quadratic complexity of standard self-attention using linear projections of keys and values. While effective at scale, **its performance in small-scale settings** (limited data, long sequences, tight compute) remains underexplored.

This project aims to:
- Understand Linformer's **strengths and failure modes** under such constraints,
- Explore how **simple architectural changes**, like adding 1D convolutions, might mitigate instability and improve performance.

---

## ðŸ“Š Phase 1: Studying Linformer Behavior

**Setup:**
- **Dataset**: [WikiText-103](https://huggingface.co/datasets/Salesforce/wikitext), subset to 50M tokens.
- **Sequence lengths**: 128, 256, 512, 1024.
- **Architecture**: Encoder-only Transformer, 8 layers, hidden size 512, 8 attention heads.
- **Training**: MLM objective, Hugging Face `Trainer`, AdamW, with warmup (10%) and weight decay (0.001).

**Findings:**
- **Longer sequences** destabilize training, especially at **higher learning rates**.
- Linformer performs **well at shorter sequence lengths**, but struggles as sequence length increases.
- Lowering the learning rate improves stability across sequence lengths.
- The instability may stem from Linformer's **low-rank approximations**, which need more training data to compensate loss of information.

---

## ðŸš€ Phase 2: Conv-Linformer

To address these limitations, I propose **Conv-Linformer**, a hybrid architecture that combines:
- **Linear projection** in early layers (preserving Linformerâ€™s efficiency and global context),
- **1D convolution** in later layers (enhancing local pattern extraction in key/value compression).

The convolution uses:
- Kernel size and stride = `n/k` (sequence length / compression size), preserving linear complexity.

**Results:**
- Conv-Linformer achieves **better training stability**, especially on longer sequences.
- It approaches Transformer-level performance while maintaining linear complexity.
- It performs **more consistently** than the original Linformer under the same constraints.

---

## ðŸ”— References

- ðŸ“„ Linformer Paper: [Linformer: Self-Attention with Linear Complexity](https://arxiv.org/abs/2006.04768)  
- ðŸ”— Linformer Code: [lucidrains/linformer](https://github.com/lucidrains/linformer)  
- ðŸ“š Dataset: [Salesforce/wikitext](https://huggingface.co/datasets/Salesforce/wikitext)

---

## ðŸŒ± Future Work

- Scale to larger datasets to evaluate generalization.
- Explore different convolution types (e.g., depthwise separable, dilated).
- Investigate downstream NLP tasks beyond masked language modeling.

